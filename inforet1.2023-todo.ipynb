{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we will work with the UN General Debate dataset. The corpus consists of 7,507 speeches held at the annual sessions of the United Nations General Assembly from 1970 to 2016. It was created in 2017 by Mikhaylov, Baturo, and Dasandi at Harvard “for understanding and measuring state preferences in world politics.” Each of the almost 200 countries in the United Nations has the opportunity to present its views on global topics such international conflicts, terrorism, or climate change at the annual General Debate.\n",
    "Work on this data is proposed in the book \n",
    "\n",
    "- https://github.com/blueprints-for-text-analytics-python/blueprints-text\n",
    "- from here, but rather it's easier to use the version on my server. \n",
    "  - https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/data/un-general-debates/un-general-debates-blueprint.csv.gz\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## downloading some toy data\n",
    "\n",
    "only once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start it only if you don't have your data yet!\n",
    "# you can also simply get the zip, unzip and put it manuaylly next to your notbook\n",
    "# https://gerdes.fr/saclay/informationRetrieval/un-general-debates-blueprint.csv.gz\n",
    "\n",
    "# !wget https://gerdes.fr/saclay/informationRetrieval/un-general-debates-blueprint.csv.gz\n",
    "# import gzip, shutil\n",
    "# with open('un-general-debates-blueprint.csv.gz', 'rb') as f_in:\n",
    "#     with gzip.open('un-general-debates-blueprint.csv', 'wb') as f_out:\n",
    "#         shutil.copyfileobj(f_in, f_out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001B[0m\r\n",
      "Collecting ipython-autotime\r\n",
      "  Downloading https://files.pythonhosted.org/packages/b4/c9/b413a24f759641bc27ef98c144b590023c8038dfb8a3f09e713e9dff12c1/ipython_autotime-0.3.1-py2.py3-none-any.whl\r\n",
      "Collecting monotonic; python_version < \"3.3\" (from ipython-autotime)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/67/7e8406a29b6c45be7af7740456f7f37025f0506ae2e05fb9009a53946860/monotonic-1.6-py2.py3-none-any.whl\r\n",
      "Collecting ipython (from ipython-autotime)\r\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/ce/2c/2849a2b37024a01a847c87d81825c0489eb22ffc6416cac009bf281ea838/ipython-5.10.0-py2-none-any.whl (760kB)\r\n",
      "\u001B[K     |████████████████████████████████| 768kB 4.3MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting simplegeneric>0.8 (from ipython->ipython-autotime)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/3d/57/4d9c9e3ae9a255cd4e1106bb57e24056d3d0709fc01b2e3e345898e49d5b/simplegeneric-0.8.1.zip\r\n",
      "Collecting pickleshare (from ipython->ipython-autotime)\r\n",
      "  Using cached https://files.pythonhosted.org/packages/9a/41/220f49aaea88bc6fa6cba8d05ecf24676326156c23b991e80b3f2fc24c77/pickleshare-0.7.5-py2.py3-none-any.whl\r\n",
      "Collecting backports.shutil-get-terminal-size; python_version == \"2.7\" (from ipython->ipython-autotime)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/cd/1750d6c35fe86d35f8562091737907f234b78fdffab42b29c72b1dd861f4/backports.shutil_get_terminal_size-1.0.0-py2.py3-none-any.whl\r\n",
      "Collecting decorator (from ipython->ipython-autotime)\r\n",
      "  Using cached https://files.pythonhosted.org/packages/ed/1b/72a1821152d07cf1d8b6fce298aeb06a7eb90f4d6d41acec9861e7cc6df0/decorator-4.4.2-py2.py3-none-any.whl\r\n",
      "Collecting pexpect; sys_platform != \"win32\" (from ipython->ipython-autotime)\r\n",
      "  Using cached https://files.pythonhosted.org/packages/39/7b/88dbb785881c28a102619d46423cb853b46dbccc70d3ac362d99773a78ce/pexpect-4.8.0-py2.py3-none-any.whl\r\n",
      "Collecting pathlib2; python_version == \"2.7\" or python_version == \"3.3\" (from ipython->ipython-autotime)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/09/eb/4af4bcd5b8731366b676192675221c5324394a580dfae469d498313b5c4a/pathlib2-2.3.7.post1-py2.py3-none-any.whl\r\n",
      "Collecting traitlets>=4.2 (from ipython->ipython-autotime)\r\n",
      "  Using cached https://files.pythonhosted.org/packages/ca/ab/872a23e29cec3cf2594af7e857f18b687ad21039c1f9b922fac5b9b142d5/traitlets-4.3.3-py2.py3-none-any.whl\r\n",
      "Collecting pygments<2.6 (from ipython->ipython-autotime)\r\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/be/39/32da3184734730c0e4d3fa3b2b5872104668ad6dc1b5a73d8e477e5fe967/Pygments-2.5.2-py2.py3-none-any.whl (896kB)\r\n",
      "\u001B[K     |████████████████████████████████| 901kB 4.4MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting appnope; sys_platform == \"darwin\" (from ipython->ipython-autotime)\r\n",
      "  Using cached https://files.pythonhosted.org/packages/41/4a/381783f26df413dde4c70c734163d88ca0550a1361cb74a1c68f47550619/appnope-0.1.3-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: setuptools>=18.5 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from ipython->ipython-autotime) (41.2.0)\r\n",
      "Collecting prompt-toolkit<2.0.0,>=1.0.4 (from ipython->ipython-autotime)\r\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/9d/d2/2f099b5cd62dab819ce7a9f1431c09a9032fbfbb6474f442722e88935376/prompt_toolkit-1.0.18-py2-none-any.whl (245kB)\r\n",
      "\u001B[K     |████████████████████████████████| 245kB 5.3MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting ptyprocess>=0.5 (from pexpect; sys_platform != \"win32\"->ipython->ipython-autotime)\r\n",
      "  Using cached https://files.pythonhosted.org/packages/22/a6/858897256d0deac81a172289110f31629fc4cee19b6f01283303e18c8db3/ptyprocess-0.7.0-py2.py3-none-any.whl\r\n",
      "Collecting six (from pathlib2; python_version == \"2.7\" or python_version == \"3.3\"->ipython->ipython-autotime)\r\n",
      "  Using cached https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl\r\n",
      "Collecting typing; python_version < \"3.5\" (from pathlib2; python_version == \"2.7\" or python_version == \"3.3\"->ipython->ipython-autotime)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/0b/cb/da856e81731833b94da70a08712f658416266a5fb2a9d9e426c8061becef/typing-3.10.0.0-py2-none-any.whl\r\n",
      "Collecting scandir; python_version < \"3.5\" (from pathlib2; python_version == \"2.7\" or python_version == \"3.3\"->ipython->ipython-autotime)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/df/f5/9c052db7bd54d0cbf1bc0bb6554362bba1012d03e5888950a4f5c5dadc4e/scandir-1.10.0.tar.gz\r\n",
      "Collecting enum34; python_version == \"2.7\" (from traitlets>=4.2->ipython->ipython-autotime)\r\n",
      "  Using cached https://files.pythonhosted.org/packages/6f/2c/a9386903ece2ea85e9807e0e062174dc26fdce8b05f216d00491be29fad5/enum34-1.1.10-py2-none-any.whl\r\n",
      "Collecting ipython-genutils (from traitlets>=4.2->ipython->ipython-autotime)\r\n",
      "  Using cached https://files.pythonhosted.org/packages/fa/bc/9bd3b5c2b4774d5f33b2d544f1460be9df7df2fe42f352135381c347c69a/ipython_genutils-0.2.0-py2.py3-none-any.whl\r\n",
      "Collecting wcwidth (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime)\r\n",
      "  Using cached https://files.pythonhosted.org/packages/20/f4/c0584a25144ce20bfcf1aecd041768b8c762c1eb0aa77502a3f0baa83f11/wcwidth-0.2.6-py2.py3-none-any.whl\r\n",
      "Collecting backports.functools-lru-cache>=1.2.1; python_version < \"3.2\" (from wcwidth->prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/c1/1a48a4bb9b515480d6c666977eeca9243be9fa9e6fb5a34be0ad9627f737/backports.functools_lru_cache-1.6.4-py2.py3-none-any.whl\r\n",
      "Installing collected packages: monotonic, simplegeneric, six, typing, scandir, pathlib2, pickleshare, backports.shutil-get-terminal-size, decorator, ptyprocess, pexpect, enum34, ipython-genutils, traitlets, pygments, appnope, backports.functools-lru-cache, wcwidth, prompt-toolkit, ipython, ipython-autotime\r\n",
      "\u001B[31mERROR: Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/monotonic.py'\r\n",
      "Consider using the `--user` option or check the permissions.\r\n",
      "\u001B[0m\r\n",
      "\u001B[33mWARNING: You are using pip version 19.2.3, however version 20.3.4 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autotime'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 3\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m----> 3\u001B[0m     \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_line_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mload_ext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mautotime\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/honlp/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2364\u001B[0m, in \u001B[0;36mInteractiveShell.run_line_magic\u001B[0;34m(self, magic_name, line, _stack_depth)\u001B[0m\n\u001B[1;32m   2363\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[0;32m-> 2364\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2365\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m/opt/anaconda3/envs/honlp/lib/python3.10/site-packages/IPython/core/magics/extension.py:33\u001B[0m, in \u001B[0;36mExtensionMagics.load_ext\u001B[0;34m(self, module_str)\u001B[0m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m UsageError(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing module name.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 33\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshell\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextension_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_extension\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule_str\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124malready loaded\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/honlp/lib/python3.10/site-packages/IPython/core/extensions.py:76\u001B[0m, in \u001B[0;36mExtensionManager.load_extension\u001B[0;34m(self, module_str)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 76\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_extension\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule_str\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/honlp/lib/python3.10/site-packages/IPython/core/extensions.py:91\u001B[0m, in \u001B[0;36mExtensionManager._load_extension\u001B[0;34m(self, module_str)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m module_str \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m sys\u001B[38;5;241m.\u001B[39mmodules:\n\u001B[0;32m---> 91\u001B[0m     mod \u001B[38;5;241m=\u001B[39m \u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule_str\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m mod \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39mmodules[module_str]\n",
      "File \u001B[0;32m/opt/anaconda3/envs/honlp/lib/python3.10/importlib/__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1050\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1027\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1004\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'autotime'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m      5\u001B[0m     get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpip install ipython-autotime\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 6\u001B[0m     \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_line_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mload_ext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mautotime\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/honlp/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2364\u001B[0m, in \u001B[0;36mInteractiveShell.run_line_magic\u001B[0;34m(self, magic_name, line, _stack_depth)\u001B[0m\n\u001B[1;32m   2362\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal_ns\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_local_scope(stack_depth)\n\u001B[1;32m   2363\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[0;32m-> 2364\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2365\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m/opt/anaconda3/envs/honlp/lib/python3.10/site-packages/IPython/core/magics/extension.py:33\u001B[0m, in \u001B[0;36mExtensionMagics.load_ext\u001B[0;34m(self, module_str)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m module_str:\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m UsageError(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing module name.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 33\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshell\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextension_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_extension\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule_str\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124malready loaded\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m extension is already loaded. To reload it, use:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m module_str)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/honlp/lib/python3.10/site-packages/IPython/core/extensions.py:76\u001B[0m, in \u001B[0;36mExtensionManager.load_extension\u001B[0;34m(self, module_str)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;124;03m\"\"\"Load an IPython extension by its module name.\u001B[39;00m\n\u001B[1;32m     70\u001B[0m \n\u001B[1;32m     71\u001B[0m \u001B[38;5;124;03mReturns the string \"already loaded\" if the extension is already loaded,\u001B[39;00m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;124;03m\"no load function\" if the module doesn't have a load_ipython_extension\u001B[39;00m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;124;03mfunction, or None if it succeeded.\u001B[39;00m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 76\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_extension\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule_str\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m:\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m module_str \u001B[38;5;129;01min\u001B[39;00m BUILTINS_EXTS:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/honlp/lib/python3.10/site-packages/IPython/core/extensions.py:91\u001B[0m, in \u001B[0;36mExtensionManager._load_extension\u001B[0;34m(self, module_str)\u001B[0m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshell\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m     90\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m module_str \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m sys\u001B[38;5;241m.\u001B[39mmodules:\n\u001B[0;32m---> 91\u001B[0m         mod \u001B[38;5;241m=\u001B[39m \u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule_str\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m     mod \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39mmodules[module_str]\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_load_ipython_extension(mod):\n",
      "File \u001B[0;32m/opt/anaconda3/envs/honlp/lib/python3.10/importlib/__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    124\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1050\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1027\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1004\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'autotime'"
     ]
    }
   ],
   "source": [
    "# this turns on the autotimer, so that every cell has a timing information below\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime\n",
    "# in order to stop using the autotimer:\n",
    "# %unload_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "from wordcloud import WordCloud\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"un-general-debates-blueprint.csv\")\n",
    "df.sample(22) #, random_state=53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get to know the data (and Pandas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns, df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the \"length\" column, describing the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df['text'].str.len()\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🚧 todo: how long did the longest speech last?\n",
    "\n",
    "length in characters: how much is one page (11pt)?  English ~ 600 words. \n",
    "\n",
    "That's approximately how many characters (including spaces)?\n",
    "\n",
    "xxxx\n",
    "\n",
    "What's your guess for \n",
    "German? French? Russian? Thai? Japanese?\n",
    "\n",
    "In English, how many words per minute? ~ 150\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many words for the longest speech?\n",
    "print(72000/6)\n",
    "# 🚧 todo:\n",
    "# how many pages for the longest speech?\n",
    "print(xxx)\n",
    "\n",
    "# how long to read one page?\n",
    "print(xxx)\n",
    "\n",
    "# how long to read the longest speech?\n",
    "print(xxx,'minutes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mean < average -> ?\n",
    "\n",
    "terms you probably know: mode ? mean ? average ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['country', 'speaker']].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaN ≠ NA\n",
    "NaN means 0/0. NaN stands for Not a Number\n",
    "\n",
    "NA is generally interpreted as a missing value and has various forms - NA_integer_, NA_real_, etc.\n",
    "\n",
    "https://stats.stackexchange.com/questions/5686/what-is-the-difference-between-nan-and-na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['position'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['speaker'].fillna('unkown', inplace=True)\n",
    "df['position'].fillna('unkown', inplace=True)\n",
    "df[df['position'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# categorical values vs numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['speaker'].str.contains('Bush')]['speaker'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'].plot(kind='box', vert=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'].plot(kind='hist', bins=30) # , figsize=(8,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel density estimation\n",
    "\n",
    "https://en.wikipedia.org/wiki/Kernel_density_estimation\n",
    "\n",
    "if error: \"FutureWarning: `distplot` is a deprecated function\"\n",
    "\n",
    "update scipy: `pip3 install --upgrade scipy `\n",
    "\n",
    "if it persists\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only if you got warnings!!!\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(8, 2))\n",
    "sns.distplot(df['length'], bins=30, kde=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seaborn docs?\n",
    "https://seaborn.pydata.org/index.html  \n",
    "https://seaborn.pydata.org/generated/seaborn.distplot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from where?\n",
    "\n",
    "catplot shows the relationship between a numerical and one or more categorical variables.\n",
    "https://seaborn.pydata.org/generated/seaborn.catplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df, x=\"country\", y=\"length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to build a selection:\n",
    "df['country'].isin(['USA', 'FRA', 'GBR', 'CHN', 'RUS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the selection\n",
    "where = df['country'].isin(['USA', 'FRA', 'GBR', 'CHN', 'RUS'])\n",
    "sns.catplot(data=df[where], x=\"country\", y=\"length\", kind='box')\n",
    "sns.catplot(data=df[where], x=\"country\", y=\"length\", kind='violin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## significant differences?\n",
    "\n",
    "Student test? Anova ?\n",
    "\n",
    "if the boxes (marking the quartiles) don't overlap each other and the sample size is at least 10, then the two groups being compared should have different medians at the 5% level: https://stats.stackexchange.com/questions/262495/reading-box-and-whisker-plots-possible-to-glean-significant-differences-between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df[where], x=\"country\", y=\"length\", kind='box', notch= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time?\n",
    "\n",
    "size() returns the number of rows per group  \n",
    "Why number of countries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('year').size().plot(title=\"Number of Countries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when more people want to speak, ...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('year').agg({'length': 'mean'}).plot(title=\"Avg. Speech Length\", ylim=(0,30000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "### 🚧 todo:\n",
    "Describe in one sentence the difference between the tokenizations. Which one is your favorite and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "text = \"Let's all together defeat last year's problem, SARS-CoV-2, in 2022!\"\n",
    "'|'.join(text.split()),len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "nochar = re.compile('\\W+')\n",
    "'|'.join(nochar.split(text)),len(nochar.split(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.\n",
    "nochar = re.compile('(\\W+)')\n",
    "'|'.join(nochar.split(text)),len(nochar.split(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.\n",
    "charorhyphen = re.compile(r'[\\w-]+')\n",
    "'|'.join(charorhyphen.findall(text)),len(charorhyphen.findall(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using a specialized class: nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.\n",
    "'|'.join(word_tokenize(text)),len(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these are idiosyncratic rules for English. Think of *viens-tu*, *où va-t-il*, *Kaffeetasse*, *cantolo*, *我爱你*, ...\n",
    "\n",
    "and it's slow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tqdm(df['text'][:100]):\n",
    "    toks = word_tokenize(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### so be patient for this line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['text'].map(word_tokenize)\n",
    "df['num_tokens'] = df['tokens'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_tokens'] = df['tokens'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where = df['country'].isin(['USA', 'FRA', 'GBR', 'CHN', 'RUS', 'FRG', 'DEU'])\n",
    "sns.catplot(data=df[where], x=\"country\", y=\"num_tokens\", kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚧 todo: When speaking English, do Germans use longer words?\n",
    "\n",
    "- Compare to English natives and French speakers using notched box plots.\n",
    "- Is the result significant?\n",
    "- How do you explain this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚧 todo:\n",
    "df['avg_wordsize'] = xxx\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚧 todo:\n",
    "where = df['country'].isin(xxx\n",
    "sns.catplot(data=df[where],xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🚧 todo:\n",
    "answer: \n",
    "xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Zipf it!\n",
    "## skim through this section if you have followed Hands-on NLP!\n",
    "but execute the code so that we have the freq_df and start again at word clouds\n",
    "### Let's first flatten the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltoks = [item for sublist in df['tokens'] for item in sublist] \n",
    "len(alltoks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Let's all together defeat last year's problem, SARS-CoV-2, in 2021!\"\n",
    "tokens = word_tokenize(text)\n",
    "counter = Counter(tokens)\n",
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the most common words of English?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(alltoks)\n",
    "counter.most_common(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for even bigger databases, it might be advisable to do the computation iteratively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "df['tokens'].map(counter.update)\n",
    "counter.most_common(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "freq_df.sort_values('freq',  inplace=True, ascending=False)\n",
    "freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df.head(22).plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df.head(2222).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df.head(2222).plot(loglog=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "futher reading:  \n",
    "https://en.wikipedia.org/wiki/Zipf's_law  \n",
    "https://stats.stackexchange.com/questions/6780/how-to-calculate-zipfs-law-coefficient-from-a-set-of-top-frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word cloud\n",
    "\n",
    "http://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud.WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.query(\"year==2015 and country=='USA'\")['text'].values[0]\n",
    "wc = WordCloud(max_words=100)\n",
    "wc.generate(text)\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1, 2, figsize=(20, 4))\n",
    "\n",
    "text = df.query(\"country=='USA'\")['text'].values[0]\n",
    "wc = WordCloud(max_words=100)\n",
    "wc.generate(text)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "text = df.query(\"country=='RUS'\")['text'].values[0]\n",
    "wc = WordCloud(max_words=100)\n",
    "wc.generate(text)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(max_words=100, stopwords=freq_df.head(50).index)\n",
    "wc.generate(text)\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `generate_from_frequencies` function allows to generate without stopwords directly from a Counter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc.generate_from_frequencies(counter)\n",
    "plt.title('from counter')\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "We want to build an inverted index:\n",
    "- make a df such that for every type, we have a 1 if the document contains the type, 0 if not.\n",
    "- for every type, give a list of document ids\n",
    "\n",
    "# 🚧 todo:\n",
    "- how many types do we have?\n",
    "- how many documents do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xxx,'types')\n",
    "print(xxx,'documents')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we are checking with a small sub-sample first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(freq_df.index[66:77])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[33:36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros((11, 3))\n",
    "A.nbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will first try the naïve way, to find out that this easily gets too slow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,t in enumerate(freq_df.index[66:77]):\n",
    "    for j,d in enumerate(df[33:36].tokens):\n",
    "        if t in d: A[i,j] =1\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros((100, 7507))\n",
    "for i,t in tqdm(enumerate(freq_df.index[:100])):\n",
    "    for j,d in enumerate(df.tokens):\n",
    "        if t in d: A[i,j] =1\n",
    "# optional (skip at first): can you do that loop more efficiently?\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.nbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚧 todo:\n",
    "\n",
    "What would be the size of the complete table?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚧 todo:\n",
    "xxx\n",
    "xxx\n",
    "# x gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚧 todo:\n",
    "\n",
    "How long will it take to fill the complete table?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚧 todo:\n",
    "# my computer takes xxx\n",
    "xxx,'seconds', xxx,'minutes', xxx,'hours'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### redoing the same thing with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df[33:36].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=freq_df.index[66:77], binary=True, min_df=0, lowercase=False)\n",
    "# understand the options: \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "X = vectorizer.fit_transform(df[33:36].text)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it pretty:\n",
    "d = {c:X.toarray()[i] for i,c in enumerate(df[33:36].index)}\n",
    "df_cv = pd.DataFrame.from_dict(d,  orient='index',columns=freq_df.index[66:77])\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trying the complete set of documents with the complete vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=freq_df.index, binary=True, min_df=0, lowercase=False)\n",
    "X = vectorizer.fit_transform(df.text)\n",
    "print(len(vectorizer.get_feature_names()))\n",
    "print(vectorizer.get_feature_names()[:11])\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- wow! comparably fast!\n",
    "\n",
    "- can you get the vector of \"the\"? is there a speech that doesn't use it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.all(X[:,1].toarray() == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a big vocabulary:\n",
    "grab a pageview file here https://dumps.wikimedia.org/other/pageviews/2022/2022-01/\n",
    "\n",
    "we produce a list of potential terms from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = []\n",
    "for li in open('pageviews-20220101-000000').read().strip().split('\\n'):\n",
    "    t=li.split()[1]\n",
    "    if li[:2]=='en' and t[:5]!='File:':\n",
    "        if t[:9]=='Category:':\n",
    "            t=t[9:] # can be improved Page:, Template:, ...\n",
    "        terms+=[t.replace('_',' ')]\n",
    "terms = sorted(set(terms))\n",
    "open('en.pages.txt','w').write('\\n'.join(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([len(t.split()) for t in terms]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t for t in terms if len(t.split())>33]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- trying to index these terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=terms, binary=True, min_df=0, lowercase=False, ngram_range=(1,4))\n",
    "X = vectorizer.fit_transform(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names()[:11])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "complete the # 🚧 todo:\n",
    "\n",
    "and\n",
    "## find the most frequently encountered Wikipedia entity\n",
    "- in number of speeches\n",
    "- in number of occurrences\n",
    "\n",
    "- which speech talks most about the \"Union of African States\"?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before submitting, check:\n",
    "- I have not imported any other modules\n",
    "- I have put explanations between the lines of code (either inline or in separate cells)\n",
    "- My notebook runs all the way through when I hit\n",
    "  1. the ↻ button and then\n",
    "  2. the ⏩︎ button (remove or comment out cells that are too slow and not needed, such as installing or downloading sections).\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
